#!/usr/bin/env python3
"""
AIæ–°é—»RSSæŠ“å–è„šæœ¬
ä»é¢„å®šä¹‰çš„RSSæºæŠ“å–AIç›¸å…³æ–°é—»
"""

import feedparser
import json
from datetime import datetime, timedelta
from typing import List, Dict
import re
import urllib.request

# RSSæ–°é—»æºé…ç½®
RSS_SOURCES = {
    "us_companies": [
        "https://openai.com/blog/rss.xml",
        "https://www.anthropic.com/news/rss",
        "https://blog.google/technology/ai/rss/",
        "https://ai.meta.com/blog/rss/",
    ],
    "tech_news": [
        "https://techcrunch.com/tag/artificial-intelligence/feed/",
        "https://www.theverge.com/ai-artificial-intelligence/rss/index.xml",
        "https://venturebeat.com/category/ai/feed/",
    ],
    "china_ai": [
        "https://www.36kr.com/feed",
        "https://www.jiqizhixin.com/rss",
    ],
}

# YouTube Channels
YOUTUBE_CHANNELS = [
    {"name": "OpenAI", "url": "https://www.youtube.com/OpenAI/videos"},
    {"name": "Anthropic", "url": "https://www.youtube.com/@anthropic-ai/videos"},
    {"name": "Google DeepMind", "url": "https://www.youtube.com/@googledeepmind/videos"},
    {"name": "Runway", "url": "https://www.youtube.com/@Runwayml/videos"},
    {"name": "Meta AI", "url": "https://www.youtube.com/@AIatMeta/videos"},
    {"name": "Lex Fridman", "url": "https://www.youtube.com/@lexfridman/videos"},
    {"name": "Lenny's Podcast", "url": "https://www.youtube.com/@LennysPodcast/videos"},
    {"name": "Peter Yang", "url": "https://www.youtube.com/@peteryangyt/videos"},
    {"name": "Dwarkesh Podcast", "url": "https://www.youtube.com/@DwarkeshPatel/videos"},
    {"name": "Unsupervised Learning", "url": "https://www.youtube.com/channel/UCUl-s_Vp-Kkk_XVyDylNwLA/videos"},
    {"name": "Training Data", "url": "https://www.youtube.com/playlist?list=PLOhHNjZItNnMm5tdW61JpnyxeYH5NDDx8"},
    {"name": "Latent Space", "url": "https://www.youtube.com/@LatentSpacePod/videos"},
    {"name": "No Priors", "url": "https://www.youtube.com/@NoPriorsPodcast/videos"},
    {"name": "Andrej Karpathy", "url": "https://www.youtube.com/@AndrejKarpathy/videos"},
    {"name": "Jeff Su", "url": "https://www.youtube.com/@JeffSu/videos"},
    {"name": "Tina Huang", "url": "https://www.youtube.com/@TinaHuang1/videos"},
    {"name": "Theoretically Media", "url": "https://www.youtube.com/@TheoreticallyMedia/videos"},
    {"name": "Matt Wolfe", "url": "https://www.youtube.com/@mreflow/videos"},
    {"name": "Matthew Berman", "url": "https://www.youtube.com/@matthewberman/videos"},
    {"name": "Curious Refuge", "url": "https://www.youtube.com/@CuriousRefuge/videos"},
    {"name": "Olivio Sarikas", "url": "https://www.youtube.com/@OlivioSarikas/videos"},
    {"name": "Riley Brown", "url": "https://www.youtube.com/@RileyBrownAI/videos"},
    {"name": "Greg Isenberg", "url": "https://www.youtube.com/@GregIsenberg/videos"},
]

# GitHub trending
# AIå…³é”®è¯è¿‡æ»¤
AI_KEYWORDS = [
    "ai", "artificial intelligence", "machine learning", "deep learning",
    "neural network", "gpt", "llm", "large language model", "chatgpt",
    "openai", "anthropic", "claude", "gemini", "meta ai", "llama",
    "é€šä¹‰", "æ–‡å¿ƒ", "æ··å…ƒ", "è±†åŒ…", "kimi", "æ™ºè°±", "å¤§æ¨¡å‹"
]


def is_ai_related(title: str, description: str = "") -> bool:
    """åˆ¤æ–­æ–°é—»æ˜¯å¦ä¸AIç›¸å…³"""
    text = (title + " " + description).lower()
    return any(keyword in text for keyword in AI_KEYWORDS)


def fetch_rss_feed(url: str, max_items: int = 10) -> List[Dict]:
    """æŠ“å–å•ä¸ªRSSæº"""
    try:
        feed = feedparser.parse(url)
        items = []

        for entry in feed.entries[:max_items]:
            # è·å–å‘å¸ƒæ—¶é—´
            published = entry.get('published_parsed') or entry.get('updated_parsed')
            if published:
                pub_date = datetime(*published[:6])
                # åªè·å–æœ€è¿‘24å°æ—¶çš„æ–°é—»
                if datetime.now() - pub_date > timedelta(days=1):
                    continue
            else:
                pub_date = None # æ— æ³•ç¡®å®šæ—¶é—´åˆ™ä¿ç•™ï¼Œåç»­å¯èƒ½è¿‡æ»¤

            title = entry.get('title', '')
            description = entry.get('summary', '') or entry.get('description', '')

            # è¿‡æ»¤AIç›¸å…³å†…å®¹
            if not is_ai_related(title, description):
                continue

            items.append({
                'title': title,
                'link': entry.get('link', ''),
                'description': clean_html(description[:200]),
                'published': pub_date.strftime('%Y-%m-%d %H:%M') if pub_date else 'æœªçŸ¥æ—¶é—´',
                'source': feed.feed.get('title', url),
            })

        return items
    except Exception as e:
        print(f"æŠ“å–RSSå¤±è´¥ {url}: {e}")
        return []


def clean_html(text: str) -> str:
    """æ¸…ç†HTMLæ ‡ç­¾"""
    text = re.sub(r'<[^>]+>', '', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()


def fetch_youtube_latest(channels: List[Dict]) -> List[Dict]:
    """ä»YouTubeé¢‘é“æŠ“å–æœ€æ–°è§†é¢‘"""
    videos = []
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }

    print("å¼€å§‹æŠ“å–YouTubeé¢‘é“...")
    for channel in channels:
        try:
            url = channel['url']
            req = urllib.request.Request(url, headers=headers)
            with urllib.request.urlopen(req, timeout=10) as response:
                html = response.read().decode('utf-8')
                
                # æå–è§†é¢‘IDå’Œæ ‡é¢˜
                # è¿™ç§æ–¹æ³•ä¾èµ–äºYouTubeé¡µé¢ç»“æ„ï¼Œå¯èƒ½ä¼šå¤±æ•ˆ
                video_ids = re.findall(r'"videoId":"([^"]+)"', html)
                # å°è¯•æ›´ç²¾ç¡®åœ°åŒ¹é…æ ‡é¢˜ï¼Œé¿å…åŒ¹é…åˆ°å…¶ä»–æ¨èè§†é¢‘
                # åœ¨åˆ—è¡¨é¡µï¼Œé€šå¸¸æ˜¯ title -> runs -> text
                titles = re.findall(r'"title":\{"runs":\[\{"text":"([^"]+)"\}\]', html)
                
                # ç®€å•çš„å»é‡å’Œå¯¹é½
                if video_ids and titles:
                    # è·å–ç¬¬ä¸€ä¸ªè§†é¢‘ï¼ˆé€šå¸¸æ˜¯æœ€æ–°ç½®é¡¶æˆ–æœ€æ–°çš„ï¼‰
                    # æ³¨æ„ï¼šYouTubeé¡µé¢æºç ä¸­çš„é¡ºåºä¸ä¸€å®šå®Œå…¨å¯¹åº”è§†è§‰é¡ºåºï¼Œä½†ç¬¬ä¸€ä¸ªvideoIdé€šå¸¸æ˜¯ä¸»è§†é¢‘
                    
                    # è¿‡æ»¤æ‰å¯èƒ½çš„æ— æ•ˆIDæˆ–å¹¿å‘Š
                    valid_videos = []
                    for vid, title in zip(video_ids, titles):
                        if len(vid) == 11: # YouTube video ID length
                            valid_videos.append((vid, title))
                    
                    if valid_videos:
                        vid_id, title = valid_videos[0]
                        # å¯¹äºæ’­æ”¾åˆ—è¡¨ï¼Œç»“æ„å¯èƒ½ä¸åŒï¼Œä½†videoIdæ€»æ˜¯å­˜åœ¨çš„
                        
                        videos.append({
                            'title': title,
                            'link': f"https://www.youtube.com/watch?v={vid_id}",
                            'description': f"æ¥è‡ª {channel['name']} çš„æœ€æ–°è§†é¢‘",
                            'published': 'Latest',
                            'source': channel['name']
                        })
                        print(f"  - {channel['name']}: {title}")
                else:
                    print(f"  - {channel['name']}: æœªæ‰¾åˆ°è§†é¢‘")

        except Exception as e:
            print(f"  - {channel['name']} æŠ“å–å¤±è´¥: {e}")
            
    return videos


def categorize_news(items: List[Dict]) -> Dict[str, List[Dict]]:
    """å°†æ–°é—»åˆ†ç±»"""
    categorized = {
        'us_companies': [],
        'china_companies': [],
        'general': [],
    }

    us_companies = ['openai', 'anthropic', 'google', 'meta', 'microsoft', 'amazon']
    china_companies = ['é˜¿é‡Œ', 'è…¾è®¯', 'å­—èŠ‚', 'ç™¾åº¦', 'kimi', 'æ™ºè°±', 'alibaba', 'tencent', 'baidu']

    for item in items:
        text = (item['title'] + ' ' + item['description']).lower()

        if any(company in text for company in us_companies):
            categorized['us_companies'].append(item)
        elif any(company in text for company in china_companies):
            categorized['china_companies'].append(item)
        else:
            categorized['general'].append(item)

    return categorized


def fetch_github_trending() -> List[Dict]:
    """é€šè¿‡GitHub APIè·å–çƒ­é—¨AIé¡¹ç›®"""
    items = []
    try:
        # æœç´¢æœ€è¿‘7å¤©åˆ›å»ºçš„ï¼Œstarsæ•°é«˜çš„ï¼Œå¸¦æœ‰aiæ ‡ç­¾çš„é¡¹ç›®
        date_7_days_ago = (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')
        # query = f"topic:ai created:>{date_7_days_ago}"
        # url = f"https://api.github.com/search/repositories?q={query}&sort=stars&order=desc&per_page=10"
        
        # ä½¿ç”¨ quote å¤„ç†ç©ºæ ¼ç­‰ç‰¹æ®Šå­—ç¬¦
        q = urllib.parse.quote(f"topic:ai created:>{date_7_days_ago}")
        url = f"https://api.github.com/search/repositories?q={q}&sort=stars&order=desc&per_page=10"
        
        headers = {
            'User-Agent': 'Mozilla/5.0',
            'Accept': 'application/vnd.github.v3+json'
        }
        
        req = urllib.request.Request(url, headers=headers)
        with urllib.request.urlopen(req, timeout=10) as response:
            data = json.loads(response.read().decode('utf-8'))
            
            for repo in data.get('items', []):
                items.append({
                    'title': repo['full_name'],
                    'link': repo['html_url'],
                    'description': repo['description'] or 'æš‚æ— æè¿°',
                    'stars': repo['stargazers_count'],
                    'today_stars': 'N/A', # APIä¸ç›´æ¥æä¾›ä»Šæ—¥æ–°å¢ï¼Œæš‚ç•¥
                    'source': 'GitHub'
                })
    except Exception as e:
        print(f"æŠ“å–GitHubå¤±è´¥: {e}")
        
    return items


def fetch_all_news() -> Dict:
    """æŠ“å–æ‰€æœ‰æ–°é—»æº"""
    all_news = {
        'us_companies': [],
        'china_companies': [],
        'github': [],
        'youtube': [], 
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'date': datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')
    }

    print("æ­£åœ¨è·å–ç¾å›½AIå…¬å¸æ–°é—»...")
    # æŠ“å–ç¾å›½å…¬å¸æ–°é—»
    for url in RSS_SOURCES['us_companies']:
        items = fetch_rss_feed(url)
        all_news['us_companies'].extend(items)

    print("æ­£åœ¨è·å–ä¸­å›½AIæ–°é—»...")
    # æŠ“å–ä¸­å›½AIæ–°é—»
    for url in RSS_SOURCES['china_ai']:
        items = fetch_rss_feed(url)
        all_news['china_companies'].extend(items)

    print("æ­£åœ¨è·å–ç§‘æŠ€åª’ä½“æ–°é—»...")
    # æŠ“å–é€šç”¨ç§‘æŠ€æ–°é—»å¹¶åˆ†ç±»
    for url in RSS_SOURCES['tech_news']:
        items = fetch_rss_feed(url)
        categorized = categorize_news(items)
        all_news['us_companies'].extend(categorized['us_companies'])
        all_news['china_companies'].extend(categorized['china_companies'])
        # é€šç”¨æ–°é—»æš‚å½’å…¥ç¾å›½/å›½é™…ç±»åˆ«ï¼Œé¿å…ä¸¢å¤±
        all_news['us_companies'].extend(categorized['general'])
        
    print("æ­£åœ¨è·å–GitHubçƒ­é—¨é¡¹ç›®...")
    all_news['github'] = fetch_github_trending()

    print("æ­£åœ¨è·å–YouTubeè§†é¢‘...")
    # æŠ“å–YouTubeæ–°é—»
    youtube_videos = fetch_youtube_latest(YOUTUBE_CHANNELS)
    all_news['youtube'].extend(youtube_videos)

    # å»é‡
    for category in ['us_companies', 'china_companies', 'youtube']:
        seen = set()
        unique_items = []
        for item in all_news.get(category, []):
            if item['link'] not in seen:
                seen.add(item['link'])
                unique_items.append(item)
        all_news[category] = unique_items[:15]  # æ¯ä¸ªç±»åˆ«æœ€å¤š15æ¡

    return all_news


def generate_html_section(items: List[Dict], type: str) -> str:
    """ç”Ÿæˆå„ä¸ªæ¿å—çš„HTML"""
    html = ""
    if not items:
        return '<div class="news-item"><p>æš‚æ— ç›¸å…³å†…å®¹</p></div>'

    for item in items:
        if type == 'news':
            # ç®€å•çš„æ ‡ç­¾æå–é€»è¾‘
            tag_class = 'tag-google' # Default
            tag_text = 'News'
            
            title_lower = item['title'].lower()
            if 'openai' in title_lower: tag_class, tag_text = 'tag-openai', 'OpenAI'
            elif 'anthropic' in title_lower or 'claude' in title_lower: tag_class, tag_text = 'tag-anthropic', 'Anthropic'
            elif 'google' in title_lower or 'gemini' in title_lower: tag_class, tag_text = 'tag-google', 'Google'
            elif 'meta' in title_lower: tag_class, tag_text = 'tag-meta', 'Meta'
            elif 'alibaba' in title_lower or 'ali' in title_lower: tag_class, tag_text = 'tag-alibaba', 'Alibaba'
            elif 'tencent' in title_lower: tag_class, tag_text = 'tag-tencent', 'Tencent'
            
            html += f"""
            <div class="news-item">
                <h3>
                    <span class="news-tag {tag_class}">{tag_text}</span>
                    <a href="{item['link']}" target="_blank">{item['title']}</a>
                </h3>
                <p>{item['description']}</p>
                <div class="news-meta">
                    <span>ğŸ“… {item['published']}</span>
                    <a href="{item['link']}" target="_blank">æ¥æº: {item['source']}</a>
                </div>
            </div>
            """
        elif type == 'github':
            html += f"""
            <div class="github-project">
                <h3>
                    <span>ğŸ“¦</span>
                    <a href="{item['link']}" target="_blank">{item['title']}</a>
                    <span class="news-tag tag-hot">ğŸ”¥ Hot</span>
                </h3>
                <p>{item['description']}</p>
                <div class="github-stats">
                    <span>â­ {item.get('stars', 0)}</span>
                </div>
            </div>
            """
        elif type == 'youtube':
            html += f"""
            <div class="youtube-video">
                <h3><a href="{item['link']}" target="_blank">{item['title']}</a></h3>
                <div class="youtube-channel">ğŸ“º {item['source']}</div>
            </div>
            """
    return html


def generate_html(news_data: Dict, output_dir: str = '.'):
    """ç”Ÿæˆæœ€ç»ˆHTMLæ–‡ä»¶"""
    import os
    
    # ç¡®å®šæ¨¡æ¿è·¯å¾„
    script_dir = os.path.dirname(os.path.abspath(__file__))
    template_path = os.path.join(script_dir, '../assets/news_template_v2.html')
    
    try:
        with open(template_path, 'r', encoding='utf-8') as f:
            template = f.read()
            
        # ç”Ÿæˆå„éƒ¨åˆ†HTML
        us_news_html = generate_html_section(news_data['us_companies'], 'news')
        cn_news_html = generate_html_section(news_data['china_companies'], 'news')
        github_html = generate_html_section(news_data['github'], 'github')
        youtube_html = generate_html_section(news_data['youtube'], 'youtube')
        
        # å†å²è®°å½•éƒ¨åˆ† (ç®€åŒ–å¤„ç†ï¼Œæš‚ä¸ºç©º)
        history_html = ""
        
        # æ›¿æ¢å ä½ç¬¦
        html = template.replace('{{DATE}}', news_data['date'])
        html = html.replace('{{US_COUNT}}', str(len(news_data['us_companies'])))
        html = html.replace('{{US_NEWS}}', us_news_html)
        html = html.replace('{{CN_COUNT}}', str(len(news_data['china_companies'])))
        html = html.replace('{{CN_NEWS}}', cn_news_html)
        html = html.replace('{{GITHUB_COUNT}}', str(len(news_data['github'])))
        html = html.replace('{{GITHUB_PROJECTS}}', github_html)
        html = html.replace('{{YOUTUBE_COUNT}}', str(len(news_data['youtube'])))
        html = html.replace('{{YOUTUBE_VIDEOS}}', youtube_html)
        html = html.replace('{{HISTORY_SECTION}}', history_html)
        
        # ä¿å­˜æ–‡ä»¶
        filename = f"ai-news-{datetime.now().strftime('%Y-%m-%d')}.html"
        output_path = os.path.join(output_dir, filename)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(html)
            
        print(f"æˆåŠŸç”Ÿæˆæ—¥æŠ¥: {output_path}")
        return output_path
        
    except Exception as e:
        print(f"ç”ŸæˆHTMLå¤±è´¥: {e}")
        return None


if __name__ == '__main__':
    news = fetch_all_news()
    # print(json.dumps(news, ensure_ascii=False, indent=2))
    generate_html(news)

